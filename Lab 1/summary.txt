
108 CHAPTER 2 • APPLICATION LAYER
a Web-based e-mail application (such as Hotmail), the browser sends cookie infor-
mation to the server, permitting the server to identify the user throughout the user’s
session with the application.
Although cookies often simplify the Internet shopping experience for the user,
they are controversial because they can also be considered as an invasion of privacy.
As we just saw, using a combination of cookies and user-supplied account informa-
tion, a Web site can learn a lot about a user and potentially sell this information to a
third party.
2.2.5 Web Caching
A Web cache—also called a proxy server—is a network entity that satisfies HTTP
requests on the behalf of an origin Web server. The Web cache has its own disk
storage and keeps copies of recently requested objects in this storage. As shown in
Figure 2.11, a user’s browser can be configured so that all of the user’s HTTP requests
are first directed to the Web cache [RFC 7234]. Once a browser is configured, each
browser request for an object is first directed to the Web cache. As an example,
suppose a browser is requesting the object http://www.someschool.edu/
campus.gif. Here is what happens:
1. The browser establishes a TCP connection to the Web cache and sends an HTTP
request for the object to the Web cache.
2. The Web cache checks to see if it has a copy of the object stored locally. If it
does, the Web cache returns the object within an HTTP response message to the
client browser.
3. If the Web cache does not have the object, the Web cache opens a TCP connec-
tion to the origin server, that is, to www.someschool.edu. The Web cache
HTTP request
HTTP response
HTTP request
HTTP response
HTTP request
HTTP response
HTTP request
HTTP response
Client Origin
server
Origin
server
Client
Proxy
server
Figure 2.11 ♦ Clients requesting objects through a Web cache
2.2 • THE WEB AND HTTP 109
then sends an HTTP request for the object into the cache-to-server TCP connec-
tion. After receiving this request, the origin server sends the object within an
HTTP response to the Web cache.
4. When the Web cache receives the object, it stores a copy in its local storage and
sends a copy, within an HTTP response message, to the client browser (over the
existing TCP connection between the client browser and the Web cache).
Note that a cache is both a server and a client at the same time. When it receives
requests from and sends responses to a browser, it is a server. When it sends requests
to and receives responses from an origin server, it is a client.
Typically a Web cache is purchased and installed by an ISP. For example, a uni-
versity might install a cache on its campus network and configure all of the campus
browsers to point to the cache. Or a major residential ISP (such as Comcast) might
install one or more caches in its network and preconfigure its shipped browsers to
point to the installed caches.
Web caching has seen deployment in the Internet for two reasons. First, a Web
cache can substantially reduce the response time for a client request, particularly if
the bottleneck bandwidth between the client and the origin server is much less than
the bottleneck bandwidth between the client and the cache. If there is a high-speed
connection between the client and the cache, as there often is, and if the cache has
the requested object, then the cache will be able to deliver the object rapidly to the
client. Second, as we will soon illustrate with an example, Web caches can sub-
stantially reduce traffic on an institution’s access link to the Internet. By reducing
traffic, the institution (for example, a company or a university) does not have to
upgrade bandwidth as quickly, thereby reducing costs. Furthermore, Web caches
can substantially reduce Web traffic in the Internet as a whole, thereby improving
performance for all applications.
To gain a deeper understanding of the benefits of caches, let’s consider an exam-
ple in the context of Figure 2.12. This figure shows two networks—the institutional
network and the rest of the public Internet. The institutional network is a high-speed
LAN. A router in the institutional network and a router in the Internet are connected
by a 15 Mbps link. The origin servers are attached to the Internet but are located all
over the globe. Suppose that the average object size is 1 Mbits and that the average
request rate from the institution’s browsers to the origin servers is 15 requests per
second. Suppose that the HTTP request messages are negligibly small and thus cre-
ate no traffic in the networks or in the access link (from institutional router to Internet
router). Also suppose that the amount of time it takes from when the router on the
Internet side of the access link in Figure 2.12 forwards an HTTP request (within an
IP datagram) until it receives the response (typically within many IP datagrams) is
two seconds on average. Informally, we refer to this last delay as the “Internet delay.”
The total response time—that is, the time from the browser’s request of an
object until its receipt of the object—is the sum of the LAN delay, the access delay
(that is, the delay between the two routers), and the Internet delay. Let’s now do
110 CHAPTER 2 • APPLICATION LAYER
Public Internet
Institutional network
15 Mbps access link
100 Mbps LAN
Origin servers
Figure 2.12 ♦ Bottleneck between an institutional network and the Internet
a very crude calculation to estimate this delay. The traffic intensity on the LAN
(see Section 1.4.2) is
(15 requests/sec) # (1 Mbits/request)/(100 Mbps) = 0.15
whereas the traffic intensity on the access link (from the Internet router to institution
router) is
(15 requests/sec) # (1 Mbits/request)/(15 Mbps) = 1
A traffic intensity of 0.15 on a LAN typically results in, at most, tens of millisec-
onds of delay; hence, we can neglect the LAN delay. However, as discussed in
Section 1.4.2, as the traffic intensity approaches 1 (as is the case of the access link
in Figure 2.12), the delay on a link becomes very large and grows without bound.
Thus, the average response time to satisfy requests is going to be on the order of
minutes, if not more, which is unacceptable for the institution’s users. Clearly
something must be done.
2.2 • THE WEB AND HTTP 111
One possible solution is to increase the access rate from 15 Mbps to, say, 100 Mbps.
This will lower the traffic intensity on the access link to 0.15, which translates to neg-
ligible delays between the two routers. In this case, the total response time will roughly
be two seconds, that is, the Internet delay. But this solution also means that the institu-
tion must upgrade its access link from 15 Mbps to 100 Mbps, a costly proposition.
Now consider the alternative solution of not upgrading the access link but
instead installing a Web cache in the institutional network. This solution is illustrated
in Figure 2.13. Hit rates—the fraction of requests that are satisfied by a cache—
typically range from 0.2 to 0.7 in practice. For illustrative purposes, let’s suppose
that the cache provides a hit rate of 0.4 for this institution. Because the clients and
the cache are connected to the same high-speed LAN, 40 percent of the requests will
be satisfied almost immediately, say, within 10 milliseconds, by the cache. Neverthe-
less, the remaining 60 percent of the requests still need to be satisfied by the origin
servers. But with only 60 percent of the requested objects passing through the access
link, the traffic intensity on the access link is reduced from 1.0 to 0.6. Typically, a
Public Internet
Institutional network
15 Mbps access link
Institutional
cache
100 Mbps LAN
Origin servers
Figure 2.13 ♦ Adding a cache to the institutional network
112 CHAPTER 2 • APPLICATION LAYER
traffic intensity less than 0.8 corresponds to a small delay, say, tens of milliseconds,
on a 15 Mbps link. This delay is negligible compared with the two-second Internet
delay. Given these considerations, average delay therefore is
0.4 # (0.01 seconds) + 0.6 # (2.01 seconds)
which is just slightly greater than 1.2 seconds. Thus, this second solution provides an
even lower response time than the first solution, and it doesn’t require the institution
to upgrade its link to the Internet. The institution does, of course, have to purchase
and install a Web cache. But this cost is low—many caches use public-domain soft-
ware that runs on inexpensive PCs.
Through the use of Content Distribution Networks (CDNs), Web caches are
increasingly playing an important role in the Internet. A CDN company installs many
geographically distributed caches throughout the Internet, thereby localizing much of
the traffic. There are shared CDNs (such as Akamai and Limelight) and dedicated CDNs
(such as Google and Netflix). We will discuss CDNs in more detail in Section 2.6.
The Conditional GET
Although caching can reduce user-perceived response times, it introduces a new
problem—the copy of an object residing in the cache may be stale. In other words,
the object housed in the Web server may have been modified since the copy was
cached at the client. Fortunately, HTTP has a mechanism that allows a cache to
verify that its objects are up to date. This mechanism is called the conditional GET
[RFC 7232]. An HTTP request message is a so-called conditional GET message if
(1) the request message uses the GET method and (2) the request message includes an
If-Modified-Since: header line.
To illustrate how the conditional GET operates, let’s walk through an example.
First, on the behalf of a requesting browser, a proxy cache sends a request message
to a Web server:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
Second, the Web server sends a response message with the requested object to the
cache:
HTTP/1.1 200 OK
Date: Sat, 3 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)
Last-Modified: Wed, 9 Sep 2015 09:23:24
Content-Type: image/gif
(data data data data data ...)
2.2 • THE WEB AND HTTP 113
The cache forwards the object to the requesting browser but also caches the object
locally. Importantly, the cache also stores the last-modified date along with the
object. Third, one week later, another browser requests the same object via the cache,
and the object is still in the cache. Since this object may have been modified at the
Web server in the past week, the cache performs an up-to-date check by issuing a
conditional GET. Specifically, the cache sends:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
If-modified-since: Wed, 9 Sep 2015 09:23:24
Note that the value of the If-modified-since: header line is exactly equal
to the value of the Last-Modified: header line that was sent by the server one
week ago. This conditional GET is telling the server to send the object only if the
object has been modified since the specified date. Suppose the object has not been
modified since 9 Sep 2015 09:23:24. Then, fourth, the Web server sends a response
message to the cache:
HTTP/1.1 304 Not Modified
Date: Sat, 10 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)
(empty entity body)
We see that in response to the conditional GET, the Web server still sends a
response message but does not include the requested object in the response message.
Including the requested object would only waste bandwidth and increase user-
perceived response time, particularly if the object is large. Note that this last response
message has 304 Not Modified in the status line, which tells the cache that it
can go ahead and forward its (the proxy cache’s) cached copy of the object to the
requesting browser.
2.2.6 HTTP/2
HTTP/2 [RFC 7540], standardized in 2015, was the first new version of HTTP since
HTTP/1.1, which was standardized in 1997. Since standardization, HTTP/2 has
taken off, with over 40% of the top 10 million websites supporting HTTP/2 in 2020
[W3Techs]. Most browsers—including Google Chrome, Internet Explorer, Safari,
Opera, and Firefox—also support HTTP/2.
The primary goals for HTTP/2 are to reduce perceived latency by enabling request
and response multiplexing over a single TCP connection, provide request prioritization
and server push, and provide efficient compression of HTTP header fields. HTTP/2
does not change HTTP methods, status codes, URLs, or header fields. Instead, HTTP/2
changes how the data is formatted and transported between the client and server.
114 CHAPTER 2 • APPLICATION LAYER
To motivate the need for HTTP/2, recall that HTTP/1.1 uses persistent TCP
connections, allowing a Web page to be sent from server to client over a single TCP
connection. By having only one TCP connection per Web page, the number of sock-
ets at the server is reduced and each transported Web page gets a fair share of the
network bandwidth (as discussed below). But developers of Web browsers quickly
discovered that sending all the objects in a Web page over a single TCP connec-
tion has a Head of Line (HOL) blocking problem. To understand HOL blocking,
consider a Web page that includes an HTML base page, a large video clip near the
top of Web page, and many small objects below the video. Further suppose there is
a low-to-medium speed bottleneck link (for example, a low-speed wireless link) on
the path between server and client. Using a single TCP connection, the video clip
will take a long time to pass through the bottleneck link, while the small objects are
delayed as they wait behind the video clip; that is, the video clip at the head of the
line blocks the small objects behind it. HTTP/1.1 browsers typically work around this
problem by opening multiple parallel TCP connections, thereby having objects in the
same web page sent in parallel to the browser. This way, the small objects can arrive
at and be rendered in the browser much faster, thereby reducing user-perceived delay.
TCP congestion control, discussed in detail in Chapter 3, also provides brows-
ers an unintended incentive to use multiple parallel TCP connections rather than a
single persistent connection. Very roughly speaking, TCP congestion control aims to
give each TCP connection sharing a bottleneck link an equal share of the available
bandwidth of that link; so if there are n TCP connections operating over a bottleneck
link, then each connection approximately gets 1/nth of the bandwidth. By opening
multiple parallel TCP connections to transport a single Web page, the browser can
“cheat” and grab a larger portion of the link bandwidth. Many HTTP/1.1 browsers
open up to six parallel TCP connections not only to circumvent HOL blocking but
also to obtain more bandwidth.
One of the primary goals of HTTP/2 is to get rid of (or at least reduce the num-
ber of) parallel TCP connections for transporting a single Web page. This not only
reduces the number of sockets that need to be open and maintained at servers, but
also allows TCP congestion control to operate as intended. But with only one TCP
connection to transport a Web page, HTTP/2 requires carefully designed mecha-
nisms to avoid HOL blocking.
HTTP/2 Framing
The HTTP/2 solution for HOL blocking is to break each message into small frames, and
interleave the request and response messages on the same TCP connection. To under-
stand this, consider again the example of a Web page consisting of one large video clip
and, say, 8 smaller objects. Thus the server will receive 9 concurrent requests from any
browser wanting to see this Web page. For each of these requests, the server needs to
send 9 competing HTTP response messages to the browser. Suppose all frames are of
2.2 • THE WEB AND HTTP 115
fixed length, the video clip consists of 1000 frames, and each of the smaller objects
consists of two frames. With frame interleaving, after sending one frame from the
video clip, the first frames of each of the small objects are sent. Then after sending the
second frame of the video clip, the last frames of each of the small objects are sent.
Thus, all of the smaller objects are sent after sending a total of 18 frames. If interleav-
ing were not used, the smaller objects would be sent only after sending 1016 frames.
Thus the HTTP/2 framing mechanism can significantly decrease user-perceived delay.
The ability to break down an HTTP message into independent frames, inter-
leave them, and then reassemble them on the other end is the single most important
enhancement of HTTP/2. The framing is done by the framing sub-layer of the
HTTP/2 protocol. When a server wants to send an HTTP response, the response
is processed by the framing sub-layer, where it is broken down into frames. The
header field of the response becomes one frame, and the body of the message is
broken down into one for more additional frames. The frames of the response are
then interleaved by the framing sub-layer in the server with the frames of other
responses and sent over the single persistent TCP connection. As the frames arrive
at the client, they are first reassembled into the original response messages at the
framing sub-layer and then processed by the browser as usual. Similarly, a client’s
HTTP requests are broken into frames and interleaved.
In addition to breaking down each HTTP message into independent frames, the
framing sublayer also binary encodes the frames. Binary protocols are more efficient
to parse, lead to slightly smaller frames, and are less error-prone.
Response Message Prioritization and Server Pushing
Message prioritization allows developers to customize the relative priority of
requests to better optimize application performance. As we just learned, the fram-
ing sub-layer organizes messages into parallel streams of data destined to the same
requestor. When a client sends concurrent requests to a server, it can prioritize the
responses it is requesting by assigning a weight between 1 and 256 to each message.
The higher number indicates higher priority. Using these weights, the server can
send first the frames for the responses with the highest priority. In addition to this,
the client also states each message’s dependency on other messages by specifying
the ID of the message on which it depends.
Another feature of HTTP/2 is the ability for a server to send multiple responses
for a single client request. That is, in addition to the response to the original request,
the server can push additional objects to the client, without the client having to
request each one. This is possible since the HTML base page indicates the objects
that will be needed to fully render the Web page. So instead of waiting for the
HTTP requests for these objects, the server can analyze the HTML page, identify
the objects that are needed, and send them to the client before receiving explicit
requests for these objects. Server push eliminates the extra latency due to waiting
for the requests.
116 CHAPTER 2 • APPLICATION LAYER
HTTP/3
QUIC, discussed in Chapter 3, is a new “transport” protocol that is implemented in
the application layer over the bare-bones UDP protocol. QUIC has several features
that are desirable for HTTP, such as message multiplexing (interleaving), per-stream
flow control, and low-latency connection establishment. HTTP/3 is yet a new HTTP
protocol that is designed to operate over QUIC. As of 2020, HTTP/3 is described
in Internet drafts and has not yet been fully standardized. Many of the HTTP/2 fea-
tures (such as message interleaving) are subsumed by QUIC, allowing for a simpler,
streamlined design for HTTP/3



2.7 • SOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS 161
TCPClient.py
Here is the code for the client side of the application:
from socket import *
serverName = ’servername’
serverPort = 12000
clientSocket = socket(AF_INET, SOCK_STREAM)
clientSocket.connect((serverName,serverPort))
sentence = input(’Input lowercase sentence:’)
clientSocket.send(sentence.encode())
modifiedSentence = clientSocket.recv(1024)
print(’From Server: ’, modifiedSentence.decode())
clientSocket.close()
Let’s now take a look at the various lines in the code that differ significantly from the
UDP implementation. The first such line is the creation of the client socket.
clientSocket = socket(AF_INET, SOCK_STREAM)
This line creates the client’s socket, called clientSocket. The first parameter
again indicates that the underlying network is using IPv4. The second parameter
Client process Server process
Client
socket
Welcoming
socket
Three-way handshake
Connection
socket
bytes
bytes
Figure 2.28 ♦ The TCPServer process has two sockets
162 CHAPTER 2 • APPLICATION LAYER
indicates that the socket is of type SOCK_STREAM, which means it is a TCP socket
(rather than a UDP socket). Note that we are again not specifying the port number of
the client socket when we create it; we are instead letting the operating system do this
for us. Now the next line of code is very different from what we saw in UDPClient:
clientSocket.connect((serverName,serverPort))
Recall that before the client can send data to the server (or vice versa) using a TCP
socket, a TCP connection must first be established between the client and server. The
Close
connectionSocket
Write reply to
connectionSocket
Read request from
connectionSocket
Create socket, port=x,
for incoming request:
Server
serverSocket =
socket()
Wait for incoming
connection request:
connectionSocket =
serverSocket.accept()
(Running on serverIP)
Client
TCP
connection setup Create socket, connect
to serverIP, port=x:
clientSocket =
socket()
Read reply from
clientSocket
Send request using
clientSocket
Close
clientSocket
Figure 2.29 ♦ The client-server application using TCP
2.7 • SOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS 163
above line initiates the TCP connection between the client and server. The parameter
of the connect() method is the address of the server side of the connection. After
this line of code is executed, the three-way handshake is performed and a TCP con-
nection is established between the client and server.
sentence = input(’Input lowercase sentence:’)
As with UDPClient, the above obtains a sentence from the user. The string
sentence continues to gather characters until the user ends the line by typing a
carriage return. The next line of code is also very different from UDPClient:
clientSocket.send(sentence.encode())
The above line sends the sentence through the client’s socket and into the TCP
connection. Note that the program does not explicitly create a packet and attach the
destination address to the packet, as was the case with UDP sockets. Instead the cli-
ent program simply drops the bytes in the string sentence into the TCP connec-
tion. The client then waits to receive bytes from the server.
modifiedSentence = clientSocket.recv(2048)
When characters arrive from the server, they get placed into the string
modifiedSentence. Characters continue to accumulate in modifiedSen-
tence until the line ends with a carriage return character. After printing the capital-
ized sentence, we close the client’s socket:
clientSocket.close()
This last line closes the socket and, hence, closes the TCP connection between the
client and the server. It causes TCP in the client to send a TCP message to TCP in
the server (see Section 3.5).
TCPServer.py
Now let’s take a look at the server program.
from socket import *
serverPort = 12000
serverSocket = socket(AF_INET,SOCK_STREAM)
serverSocket.bind((’’,serverPort))
serverSocket.listen(1)
print(’The server is ready to receive’)
164 CHAPTER 2 • APPLICATION LAYER
while True:
connectionSocket, addr = serverSocket.accept()
sentence = connectionSocket.recv(1024).decode()
capitalizedSentence = sentence.upper()
connectionSocket.send(capitalizedSentence.encode())
connectionSocket.close()
Let’s now take a look at the lines that differ significantly from UDPServer and TCP-
Client. As with TCPClient, the server creates a TCP socket with:
serverSocket=socket(AF_INET,SOCK_STREAM)
Similar to UDPServer, we associate the server port number, serverPort, with
this socket:
serverSocket.bind((’’,serverPort))
But with TCP, serverSocket will be our welcoming socket. After establish-
ing this welcoming door, we will wait and listen for some client to knock on the
door:
serverSocket.listen(1)
This line has the server listen for TCP connection requests from the client. The
parameter specifies the maximum number of queued connections (at least 1).
connectionSocket, addr = serverSocket.accept()
When a client knocks on this door, the program invokes the accept() method for
serverSocket, which creates a new socket in the server, called connectionSocket,
dedicated to this particular client. The client and server then complete the hand-
shaking, creating a TCP connection between the client’s clientSocket and the
server’s connectionSocket. With the TCP connection established, the client
and server can now send bytes to each other over the connection. With TCP, all bytes
sent from one side are only guaranteed to arrive at the other side but also guaranteed
to arrive in order.
connectionSocket.close()
In this program, after sending the modified sentence to the client, we close the con-
nection socket. But since serverSocket remains open, another client can now
knock on the door and send the server a sentence to modify.
2.8 • SUMMARY 165
This completes our discussion of socket programming in TCP. You are encour-
aged to run the two programs in two separate hosts, and also to modify them to
achieve slightly different goals. You should compare the UDP program pair with the
TCP program pair and see how they differ. You should also do many of the socket
programming assignments described at the ends of Chapter 2, 4, and 9. Finally, we
hope someday, after mastering these and more advanced socket programs, you will
write your own popular network application, become very rich and famous, and
remember the authors of this textbook!
2.8 Summary
In this chapter, we’ve studied the conceptual and the implementation aspects of
network applications. We’ve learned about the ubiquitous client-server architec-
ture adopted by many Internet applications and seen its use in the HTTP, SMTP,
and DNS protocols. We’ve studied these important application-level protocols,
and their corresponding associated applications (the Web, file transfer, e-mail, and
DNS) in some detail. We’ve learned about the P2P architecture and contrasted it
with the client-server architecture. We’ve also learned about streaming video, and
how modern video distribution systems leverage CDNs. We’ve examined how the
socket API can be used to build network applications. We’ve walked through the
use of sockets for connection-oriented (TCP) and connectionless (UDP) end-to-end
transport services. The first step in our journey down the layered network architec-
ture is now complete!
At the very beginning of this book, in Section 1.1, we gave a rather vague, bare-
bones definition of a protocol: “the format and the order of messages exchanged
between two or more communicating entities, as well as the actions taken on the
transmission and/or receipt of a message or other event.” The material in this chapter,
and in particular our detailed study of the HTTP, SMTP, and DNS protocols, has
now added considerable substance to this definition. Protocols are a key concept in
networking; our study of application protocols has now given us the opportunity to
develop a more intuitive feel for what protocols are all about.
In Section 2.1, we described the service models that TCP and UDP offer to
applications that invoke them. We took an even closer look at these service models
when we developed simple applications that run over TCP and UDP in Section 2.7.
However, we have said little about how TCP and UDP provide these service models.
For example, we know that TCP provides a reliable data service, but we haven’t said
yet how it does so. In the next chapter, we’ll take a careful look at not only the what,
but also the how and why of transport protocols.
Equipped with knowledge about Internet application structure and application-
level protocols, we’re now ready to head further down the protocol stack and exam-
ine the transport layer in Chapter 3